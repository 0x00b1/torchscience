"""Lattice quantization operators."""

from __future__ import annotations

import torch
from torch import Tensor


def lattice_quantize(
    x: Tensor,
    lattice: str = "Zn",
    *,
    scale: float = 1.0,
    gradient_mode: str = "ste",
) -> tuple[Tensor, Tensor]:
    """Quantize vectors using lattice structures.

    Lattice quantization maps continuous vectors to the nearest point
    in a lattice, providing structured quantization for multi-dimensional
    signals with good rate-distortion properties.

    Parameters
    ----------
    x : Tensor
        Input tensor. Shape: ``(..., d)`` where ``d`` is the dimension
        of the lattice.
    lattice : {"Zn", "Dn", "An", "E8"}, default="Zn"
        Lattice type:
        - "Zn": Integer lattice (cubic). Any dimension.
        - "Dn": Checkerboard lattice (even coordinate sums). Dim >= 2.
        - "An": Simplex lattice (zero-sum hyperplane). Dim >= 2 (embeds n in n+1).
        - "E8": E8 lattice (8-dimensional, optimal packing).
    scale : float, default=1.0
        Lattice scaling factor. Larger scale = coarser quantization.
    gradient_mode : {"ste", "soft", "none"}, default="ste"
        Gradient approximation method:
        - "ste": Straight-through estimator (pass gradients through).
        - "soft": Soft quantization using temperature annealing.
        - "none": No gradients through quantization.

    Returns
    -------
    quantized : Tensor
        Quantized vectors on the lattice. Same shape as input.
    indices : Tensor
        Encoding of lattice points (implementation-specific).
        For Zn: the integer coordinates. Shape: ``(..., d)``.
        For others: packed representation.

    Examples
    --------
    >>> import torch
    >>> x = torch.randn(100, 3)
    >>> q, idx = lattice_quantize(x, lattice="Zn")

    Notes
    -----
    Lattice quantization generalizes scalar quantization to multiple
    dimensions. Key lattices for compression:

    - **Zn**: Simple but suboptimal. MSE = scaleÂ²/12 per dimension.
    - **Dn**: 1.5dB gain over Zn in high dimensions.
    - **E8**: Optimal in 8D, used in some codecs.

    The quantization gain of a lattice measures how much better it
    performs compared to scalar quantization.

    See Also
    --------
    scalar_quantize : 1D scalar quantization.
    vector_quantize : Learned codebook quantization.
    """
    if not isinstance(x, Tensor):
        raise TypeError(f"x must be a Tensor, got {type(x).__name__}")

    if x.dim() < 1:
        raise ValueError(f"x must be at least 1D, got {x.dim()}D")

    valid_lattices = {"Zn", "Dn", "An", "E8"}
    if lattice not in valid_lattices:
        raise ValueError(
            f"lattice must be one of {valid_lattices}, got '{lattice}'"
        )

    valid_modes = {"ste", "soft", "none"}
    if gradient_mode not in valid_modes:
        raise ValueError(
            f"gradient_mode must be one of {valid_modes}, got '{gradient_mode}'"
        )

    d = x.shape[-1]

    # Check dimension requirements
    if lattice == "E8" and d != 8:
        raise ValueError(f"E8 lattice requires dimension 8, got {d}")
    if lattice in ("Dn", "An") and d < 2:
        raise ValueError(f"{lattice} lattice requires dimension >= 2, got {d}")

    # Scale input
    x_scaled = x / scale

    # Quantize based on lattice type
    if lattice == "Zn":
        quantized, indices = _quantize_zn(x_scaled)
    elif lattice == "Dn":
        quantized, indices = _quantize_dn(x_scaled)
    elif lattice == "An":
        quantized, indices = _quantize_an(x_scaled)
    elif lattice == "E8":
        quantized, indices = _quantize_e8(x_scaled)

    # Apply gradient mode
    if gradient_mode == "ste":
        # Straight-through: forward uses quantized, backward passes through
        quantized = x_scaled + (quantized - x_scaled).detach()
    elif gradient_mode == "none":
        quantized = quantized.detach()
    # "soft" mode: use quantized directly (already computed)

    # Scale back
    quantized = quantized * scale

    return quantized, indices


def _quantize_zn(x: Tensor) -> tuple[Tensor, Tensor]:
    """Quantize to integer lattice Zn (round each coordinate)."""
    indices = x.round().long()
    quantized = indices.float()
    return quantized, indices


def _quantize_dn(x: Tensor) -> tuple[Tensor, Tensor]:
    """Quantize to checkerboard lattice Dn.

    Dn consists of integer points where coordinate sum is even.
    """
    # Round to nearest integers
    rounded = x.round()

    # Check if sum is even
    coord_sum = rounded.sum(dim=-1, keepdim=True)
    is_even = (coord_sum % 2 == 0).float()

    # For odd sums, flip the coordinate closest to 0.5
    fractional = x - x.floor()
    dist_to_half = (fractional - 0.5).abs()
    min_dist_idx = dist_to_half.argmin(dim=-1, keepdim=True)

    # Create adjustment: flip sign of rounding for the closest-to-0.5 coordinate
    adjustment = torch.zeros_like(rounded)
    # When coord is above floor (frac > 0.5), we rounded up; flip means round down (-1)
    # When coord is below (frac < 0.5), we rounded down; flip means round up (+1)
    flip_direction = torch.where(
        fractional.gather(-1, min_dist_idx) > 0.5,
        torch.tensor(-1.0, device=x.device),
        torch.tensor(1.0, device=x.device),
    )
    adjustment.scatter_(-1, min_dist_idx, flip_direction)

    # Apply adjustment only when sum is odd
    quantized = rounded + adjustment * (1 - is_even)

    indices = quantized.long()
    return quantized, indices


def _quantize_an(x: Tensor) -> tuple[Tensor, Tensor]:
    """Quantize to simplex lattice An.

    An consists of integer points in (n+1) dimensions that sum to zero.
    For n-dimensional input, we embed in (n+1)D, quantize, then project.
    """
    # Embed: add coordinate to make zero-sum
    last_coord = -x.sum(dim=-1, keepdim=True)
    x_embedded = torch.cat([x, last_coord], dim=-1)

    # Round to nearest integer
    rounded = x_embedded.round()

    # Adjust to make sum exactly zero
    residual = rounded.sum(dim=-1, keepdim=True)

    # Find which coordinates to adjust
    # Adjust the ones with smallest rounding error in the direction we need
    frac = x_embedded - x_embedded.floor()
    if residual.abs().max() > 0:
        # Need to reduce/increase some coordinates
        # Sort by how close they are to the boundary we need to cross
        # If residual > 0, need to reduce some (round down instead of up)
        # If residual < 0, need to increase some (round up instead of down)
        adjustment_direction = -torch.sign(residual)

        # How much did we need to move in that direction?
        # If we need to go down (residual > 0): prefer coords that rounded up (frac > 0.5)
        # If we need to go up (residual < 0): prefer coords that rounded down (frac < 0.5)
        preference = torch.where(
            adjustment_direction > 0,
            frac,  # Higher frac = closer to rounding up = easier to push up
            1
            - frac,  # Higher (1-frac) = closer to rounding down = easier to push down
        )

        # Sort and take top |residual| indices
        n_adjust = residual.abs().long()
        _, sorted_idx = preference.sort(dim=-1, descending=True)

        # Create mask for top n_adjust elements
        positions = torch.arange(x_embedded.shape[-1], device=x.device)
        mask = positions.unsqueeze(0) < n_adjust

        # Apply adjustment
        adjustment = torch.zeros_like(rounded)
        adjustment.scatter_(
            -1, sorted_idx, mask.float() * adjustment_direction
        )
        rounded = rounded + adjustment

    # Project back: drop last coordinate
    quantized = rounded[..., :-1]
    indices = quantized.long()

    return quantized, indices


def _quantize_e8(x: Tensor) -> tuple[Tensor, Tensor]:
    """Quantize to E8 lattice.

    E8 is the union of D8 and D8 + (1/2, ..., 1/2).
    """
    # Quantize to D8
    q_d8, _ = _quantize_dn(x)

    # Quantize to D8 + half
    half = torch.full_like(x, 0.5)
    q_d8_half, _ = _quantize_dn(x - half)
    q_d8_half = q_d8_half + half

    # Choose closer one
    dist_d8 = ((x - q_d8) ** 2).sum(dim=-1, keepdim=True)
    dist_d8_half = ((x - q_d8_half) ** 2).sum(dim=-1, keepdim=True)

    use_d8 = dist_d8 <= dist_d8_half
    quantized = torch.where(use_d8, q_d8, q_d8_half)

    # For indices, pack which coset (0 or 1) plus the D8 coordinates
    # Simplified: just return the quantized values as float indices
    # (E8 has 2 cosets, so we could use a flag + D8 index)
    indices = quantized.long()  # Approximate - loses half-integer info

    return quantized, indices
